provided by kimi-k2:
Here is a logical step-by-step plan for your RAG system, structured for diagram creation:

---

### **Phase 1: Infrastructure Foundation**
1. **Provision VPS**
   - Select GPU-enabled VPS (or high-CPU for quantized models)
   - Install OS (Ubuntu 22.04 LTS recommended)
   - Configure SSH access and firewall rules

2. **Install Core Dependencies**
   - Install Docker & Docker Compose
   - Install NVIDIA drivers + CUDA toolkit (if using GPU)
   - Install Python 3.10+ and pip

3. **Set up Storage Volumes**
   - Create persistent volumes for:
     - Document storage (`/data/documents`)
     - Weaviate data (`/data/weaviate`)
     - Model cache (`/data/models`)

---

### **Phase 2: Model Deployment Layer**
4. **Deploy Embedding Model**
   - Pull Hakim model: `MCINext/Hakim`
   - Run inference server (e.g., Hugging Face TEI or custom FastAPI service)
   - Expose endpoint: `http://localhost:8080/embed`

5. **Deploy Generative LLM**
   - Choose model (e.g., Llama-2-7B, Mistral-7B)
   - Run inference server (vLLM, text-generation-webui, or llama.cpp)
   - Expose endpoint: `http://localhost:8081/generate`

---

### **Phase 3: Vector Database Layer**
6. **Deploy Weaviate**
   - Create Docker Compose service for Weaviate
   - Configure schema for `DocumentChunk` class:
     - `content` (text)
     - `embedding` (vector, 768-dim for Hakim)
     - `metadata` (source, page, timestamp)
   - Start Weaviate on port `8082`

---

### **Phase 4: Application Core Layer**
7. **Build Flask Application Skeleton**
   - Create app structure: `/app`, `/config`, `/services`
   - Set up environment variables file (`.env`):
     - Weaviate URL, model endpoints, API keys

8. **Integrate LlamaIndex**
   - Install LlamaIndex packages
   - Create `WeaviateVectorStore` connector
   - Configure `HakimEmbedding` class (custom or use LangChain adapter)
   - Configure `LLM` interface to connect to your generative model

---

### **Phase 5: Data Ingestion Pipeline**
9. **Create Ingestion Service**
   - Build document loader (PDF, TXT, web scraping)
   - Implement text splitter (500 chars, 50 overlap)
   - Create embedding batch processor
   - Build Weaviate indexer with metadata enrichment
   - **Endpoint**: `POST /api/ingest`

---

### **Phase 6: Query Processing Pipeline**
10. **Build Query Engine**
    - Create query embedding function
    - Implement Weaviate hybrid search (vector + BM25)
    - Add reranking and context assembly
    - Build prompt template for LLM
    - Create response synthesizer
    - **Endpoint**: `POST /api/query`

---

### **Phase 7: API & Interface Layer**
11. **Create Flask Endpoints**
    - `/api/query` - Main RAG query endpoint (streaming support)
    - `/api/upload` - Document upload endpoint
    - `/api/delete` - Delete documents by source
    - `/api/health` - System health check

12. **Add Reverse Proxy (Nginx/Caddy)**
    - Expose Flask app on port 80/443
    - Configure SSL certificates
    - Set up rate limiting

---

### **Phase 8: Operationalization**
13. **Implement Monitoring**
    - Add logging (documents ingested, query latency, retrieved chunks)
    - Create Grafana dashboard for metrics
    - Set up health checks for all services

14. **Testing & Validation**
    - Test embedding generation speed
    - Test retrieval accuracy with sample Farsi queries
    - Test end-to-end response quality
    - Load test API endpoints

---

### **Diagram Connection Flow**
**Data Flow**: Documents → Ingestion Service → Embedding Model → Weaviate

**Query Flow**: User → Nginx → Flask → LlamaIndex → Weaviate → Context + LLM → Response

**Infrastructure Flow**: VPS → Docker → (Weaviate Container + Model Containers + Flask Container)

---

This plan gives you 14 sequential steps that can be drawn as layered architecture (bottom: VPS Infrastructure → middle: Models & Database → top: Application & API) with clear data flows connecting them.